{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtK__R-rgA4R",
        "outputId": "317a9010-a613-4678-a267-105281e901a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Collecting stable_baselines3\n",
            "  Downloading stable_baselines3-2.2.1-py3-none-any.whl (181 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.7/181.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n",
            "Collecting gymnasium<0.30,>=0.28.1 (from stable_baselines3)\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.1.0+cu121)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium<0.30,>=0.28.1->stable_baselines3)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable_baselines3) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable_baselines3) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable_baselines3) (1.3.0)\n",
            "Installing collected packages: farama-notifications, gymnasium, stable_baselines3\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1 stable_baselines3-2.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install gym stable_baselines3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVciU-JlqJPc",
        "outputId": "07e56ec2-5922-43da-ac47-7259ed41ee0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: stable-baselines3[extra] in /usr/local/lib/python3.10/dist-packages (2.2.1)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.1.0+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (3.7.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.8.0.76)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.5.2)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.15.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.66.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (13.7.0)\n",
            "Collecting shimmy[atari]~=1.3.0 (from stable-baselines3[extra])\n",
            "  Downloading Shimmy-1.3.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (9.4.0)\n",
            "Collecting autorom[accept-rom-license]~=0.6.1 (from stable-baselines3[extra])\n",
            "  Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2.31.0)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (0.0.4)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]~=1.3.0->stable-baselines3[extra])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.60.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.5.1)\n",
            "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2023.3.post1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (2.16.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]~=1.3.0->stable-baselines3[extra]) (6.1.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (3.2.2)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446660 sha256=61ea886a6ab05a3e22ace0ce2eddecccbc7f6d8d55a662cac0215444018f1953\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: ale-py, shimmy, AutoROM.accept-rom-license, autorom\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.6.1 shimmy-1.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install stable-baselines3[extra] gym\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQDoHaQigr4g",
        "outputId": "1a824b52-8d19-4c04-b3aa-2c06d69cb091"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: shimmy>=0.2.1 in /usr/local/lib/python3.10/dist-packages (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from shimmy>=0.2.1) (1.23.5)\n",
            "Requirement already satisfied: gymnasium>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from shimmy>=0.2.1) (0.29.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy>=0.2.1) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy>=0.2.1) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy>=0.2.1) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install 'shimmy>=0.2.1'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZbCN3Uifvt8"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "class JobSchedulingEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(JobSchedulingEnv, self).__init__()\n",
        "        self.num_jobs = 6\n",
        "        self.job_durations = [2, 3, 5, 6, 2, 3]\n",
        "        self.action_space = spaces.Discrete(self.num_jobs)\n",
        "        self.observation_space = spaces.MultiBinary(self.num_jobs)\n",
        "        self.max_steps = 100  # Maximum number of steps per episode\n",
        "        self.reset()\n",
        "\n",
        "    def step(self, action):\n",
        "        # Toggle the machine assignment for the selected job\n",
        "        self.state[action] = 1 - self.state[action]\n",
        "\n",
        "        # Calculate the makespan for each machine\n",
        "        makespan_m1 = sum([duration for i, duration in enumerate(self.job_durations) if self.state[i] == 0])\n",
        "        makespan_m2 = sum([duration for i, duration in enumerate(self.job_durations) if self.state[i] == 1])\n",
        "        new_makespan = max(makespan_m1, makespan_m2)\n",
        "\n",
        "        # Calculate reward based on the change in makespan\n",
        "        reward = self.current_makespan - new_makespan\n",
        "        self.current_makespan = new_makespan\n",
        "\n",
        "        # Increment the step count and check for termination\n",
        "        self.current_step += 1\n",
        "        done = self.current_step >= self.max_steps\n",
        "\n",
        "        return self.state, reward, done, {}\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = np.zeros(self.num_jobs, dtype=int)\n",
        "        self.current_makespan = sum(self.job_durations)\n",
        "        self.current_step = 0\n",
        "        return self.state\n",
        "\n",
        "    def render(self, mode='human', close=False):\n",
        "        if close:\n",
        "            return\n",
        "\n",
        "        # Assign jobs to each machine based on the current state\n",
        "        jobs_on_m1 = [f\"J{i+1}\" for i in range(self.num_jobs) if self.state[i] == 0]\n",
        "        jobs_on_m2 = [f\"J{i+1}\" for i in range(self.num_jobs) if self.state[i] == 1]\n",
        "\n",
        "        # Calculate makespan for each machine\n",
        "        makespan_m1 = sum([duration for i, duration in enumerate(self.job_durations) if self.state[i] == 0])\n",
        "        makespan_m2 = sum([duration for i, duration in enumerate(self.job_durations) if self.state[i] == 1])\n",
        "\n",
        "        # Print the scheduling status\n",
        "        print(f\"Machine 1 (M1) - Jobs: {', '.join(jobs_on_m1)} | Makespan: {makespan_m1} minutes\")\n",
        "        print(f\"Machine 2 (M2) - Jobs: {', '.join(jobs_on_m2)} | Makespan: {makespan_m2} minutes\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    # Method to set a specific initial state (optional)\n",
        "    def set_initial_state(self, initial_state):\n",
        "        if len(initial_state) == self.num_jobs:\n",
        "            self.state = np.array(initial_state, dtype=int)\n",
        "\n",
        "# Initialize the environment and the model\n",
        "env = JobSchedulingEnv()\n",
        "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
        "\n",
        "# Train the model\n",
        "model.learn(total_timesteps=10000)\n",
        "\n",
        "# Optionally, set a specific problem before testing\n",
        "# env.set_initial_state([0, 0, 1, 1, 0, 1]) # Example initial state\n",
        "\n",
        "# Test the trained agent\n",
        "obs = env.reset()\n",
        "for i in range(1000):\n",
        "    action, _states = model.predict(obs, deterministic=True)\n",
        "    obs, rewards, dones, info = env.step(action)\n",
        "    if dones:\n",
        "        obs = env.reset()\n",
        "    env.render()\n",
        "\n",
        "# Save the model\n",
        "model.save(\"job_scheduling_model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQWS130sqIjh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbCZU5n2op1f"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3 import PPO\n",
        "\n",
        "# Load the trained model\n",
        "model = PPO.load(\"job_scheduling_model\")\n",
        "\n",
        "# Update the job durations for the new problem\n",
        "new_job_durations = [2.30, 4.12, 7, 6, 2, 3]  # Replace with your job durations\n",
        "\n",
        "# Create a new environment with the updated job durations\n",
        "class NewJobSchedulingEnv(JobSchedulingEnv):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.job_durations = new_job_durations  # Update the job durations\n",
        "\n",
        "# Initialize the new environment\n",
        "new_env = NewJobSchedulingEnv()\n",
        "\n",
        "# Initialize variables to track the optimal solution\n",
        "optimal_makespan = float('inf')\n",
        "optimal_state = None\n",
        "\n",
        "# Test the trained agent on the new environment\n",
        "obs = new_env.reset()\n",
        "for i in range(1000):\n",
        "    action, _states = model.predict(obs, deterministic=True)\n",
        "    obs, rewards, dones, info = new_env.step(action)\n",
        "\n",
        "    # Check if the current solution is better than the best found so far\n",
        "    if new_env.current_makespan < optimal_makespan:\n",
        "        optimal_makespan = new_env.current_makespan\n",
        "        optimal_state = obs.copy()\n",
        "\n",
        "    if dones:\n",
        "        obs = new_env.reset()\n",
        "\n",
        "# Print the optimal solution\n",
        "jobs_on_m1 = [f\"J{i+1}\" for i in range(new_env.num_jobs) if optimal_state[i] == 0]\n",
        "jobs_on_m2 = [f\"J{i+1}\" for i in range(new_env.num_jobs) if optimal_state[i] == 1]\n",
        "\n",
        "print(\"Optimal Solution:\")\n",
        "print(f\"Machine 1 (M1) - Jobs: {', '.join(jobs_on_m1)} | Makespan: {sum(new_env.job_durations[i] for i in range(new_env.num_jobs) if optimal_state[i] == 0)} minutes\")\n",
        "print(f\"Machine 2 (M2) - Jobs: {', '.join(jobs_on_m2)} | Makespan: {sum(new_env.job_durations[i] for i in range(new_env.num_jobs) if optimal_state[i] == 1)} minutes\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HP6haz27OLyF"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import random\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "class JobSchedulingEnv(gym.Env):\n",
        "    def __init__(self, num_jobs=6, job_durations=[2, 3, 5, 6, 2, 3], num_machines=2):\n",
        "        super(JobSchedulingEnv, self).__init__()\n",
        "        self.num_jobs = num_jobs\n",
        "        self.job_durations = job_durations\n",
        "        self.num_machines = num_machines\n",
        "\n",
        "        # Action space: Each element in the action array represents a job's assigned machine\n",
        "        self.action_space = spaces.MultiDiscrete([num_machines] * num_jobs)\n",
        "\n",
        "        # Observation space: Each job's current machine assignment\n",
        "        self.observation_space = spaces.MultiDiscrete([num_machines] * num_jobs)\n",
        "\n",
        "        self.max_steps = 2000\n",
        "        self.reset()\n",
        "\n",
        "    def step(self, action):\n",
        "        # Save the previous maximum makespan before updating the state\n",
        "        prev_max_makespan = max([sum(self.job_durations[j] for j in range(self.num_jobs) if self.state[j] == m) for m in range(self.num_machines)])\n",
        "\n",
        "        # Update the state based on the action\n",
        "        self.state = action\n",
        "\n",
        "        # Calculate the new makespan for each machine\n",
        "        makespans = [sum(self.job_durations[j] for j in range(self.num_jobs) if self.state[j] == m) for m in range(self.num_machines)]\n",
        "        new_max_makespan = max(makespans)\n",
        "\n",
        "        # Calculate reward based on the change in the maximum makespan\n",
        "        reward = prev_max_makespan - new_max_makespan\n",
        "\n",
        "        # Increment the step count and check if the episode is done\n",
        "        self.current_step += 1\n",
        "        done = self.current_step >= self.max_steps\n",
        "\n",
        "        return self.state, reward, done, {}\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = np.zeros(self.num_jobs, dtype=int)\n",
        "        self.current_makespan = sum(self.job_durations)\n",
        "        self.current_step = 0\n",
        "        return self.state\n",
        "\n",
        "    def render(self, mode='human', close=False):\n",
        "        if close:\n",
        "            return\n",
        "\n",
        "        for m in range(self.num_machines):\n",
        "            jobs_on_machine = [f\"J{i+1}\" for i in range(self.num_jobs) if self.state[i] == m]\n",
        "            makespan = sum(self.job_durations[i] for i in range(self.num_jobs) if self.state[i] == m)\n",
        "            print(f\"Machine {m+1} - Jobs: {', '.join(jobs_on_machine)} | Makespan: {makespan} minutes\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "\n",
        "\n",
        "number_of_epochs = 10  # Define the number of epochs\n",
        "timesteps_per_epoch = 2000  # Define the number of timesteps per epoch\n",
        "num_jobs = 6\n",
        "num_machines = 3\n",
        "\n",
        "# Initialize the environment with initial job durations\n",
        "initial_job_durations = [random.uniform(1, 12) for _ in range(num_jobs)]\n",
        "env = JobSchedulingEnv(num_jobs=num_jobs, job_durations=initial_job_durations, num_machines=num_machines)\n",
        "env = make_vec_env(lambda: env, n_envs=1)\n",
        "\n",
        "# Initialize the model\n",
        "model = PPO(\"MlpPolicy\", env, learning_rate=0.00025, n_steps=2048, batch_size=64,\n",
        "            gamma=0.99, gae_lambda=0.95, clip_range=0.2, ent_coef=0.01,\n",
        "            verbose=1, tensorboard_log=\"./ppo_job_scheduling_tensorboard/\")\n",
        "\n",
        "# Train the model over multiple epochs with different job durations\n",
        "for epoch in range(number_of_epochs):\n",
        "    # Generate new job durations for this epoch\n",
        "    new_job_durations = [random.uniform(1, 12) for _ in range(num_jobs)]\n",
        "\n",
        "    # Update the environment with new job durations\n",
        "    env.envs[0].env.job_durations = new_job_durations\n",
        "\n",
        "    # Continue training the model\n",
        "    model.learn(total_timesteps=timesteps_per_epoch)\n",
        "\n",
        "    # Optional: Save the model after each epoch\n",
        "model_filename = f\"job_scheduling_model_epoch_{num_machines}machines_{num_jobs}jobs\"\n",
        "model.save(model_filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8z2CrDJsvV1"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import random\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3 import PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwbg4J5FWC6l",
        "outputId": "cd1b9ffa-2a97-425d-c5cd-1a881f1090e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 20.6     |\n",
            "| time/              |          |\n",
            "|    fps             | 230      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 8        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_3\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 20.6     |\n",
            "| time/              |          |\n",
            "|    fps             | 230      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 8        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_4\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 21.5     |\n",
            "| time/              |          |\n",
            "|    fps             | 228      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 8        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_5\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 8.89     |\n",
            "| time/              |          |\n",
            "|    fps             | 226      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 9        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_6\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 21.5     |\n",
            "| time/              |          |\n",
            "|    fps             | 227      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 8        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_7\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 21.4     |\n",
            "| time/              |          |\n",
            "|    fps             | 228      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 8        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_8\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 14.2     |\n",
            "| time/              |          |\n",
            "|    fps             | 229      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 8        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_9\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 13.3     |\n",
            "| time/              |          |\n",
            "|    fps             | 224      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 9        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_10\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 21.4     |\n",
            "| time/              |          |\n",
            "|    fps             | 225      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 9        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_11\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 14       |\n",
            "| time/              |          |\n",
            "|    fps             | 224      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 9        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_12\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 14.8     |\n",
            "| time/              |          |\n",
            "|    fps             | 226      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 9        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_13\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 21.4     |\n",
            "| time/              |          |\n",
            "|    fps             | 228      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 8        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_14\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 19.8     |\n",
            "| time/              |          |\n",
            "|    fps             | 227      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 8        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_15\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 14.7     |\n",
            "| time/              |          |\n",
            "|    fps             | 230      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 8        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_16\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 20       |\n",
            "| time/              |          |\n",
            "|    fps             | 236      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 8        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_17\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 14       |\n",
            "| time/              |          |\n",
            "|    fps             | 238      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 8        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_18\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 19.8     |\n",
            "| time/              |          |\n",
            "|    fps             | 237      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 8        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_19\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 5.81     |\n",
            "| time/              |          |\n",
            "|    fps             | 239      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 8        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_20\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 19.8     |\n",
            "| time/              |          |\n",
            "|    fps             | 239      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 8        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_21\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 19.8     |\n",
            "| time/              |          |\n",
            "|    fps             | 238      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 8        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_22\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 16.2     |\n",
            "| time/              |          |\n",
            "|    fps             | 240      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 8        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_23\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 22.2     |\n",
            "| time/              |          |\n",
            "|    fps             | 237      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 8        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_24\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 20.6     |\n",
            "| time/              |          |\n",
            "|    fps             | 232      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 8        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_25\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 22.2     |\n",
            "| time/              |          |\n",
            "|    fps             | 229      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 8        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_26\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 14       |\n",
            "| time/              |          |\n",
            "|    fps             | 227      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 9        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_27\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 7.47     |\n",
            "| time/              |          |\n",
            "|    fps             | 225      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 9        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_28\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 15.6     |\n",
            "| time/              |          |\n",
            "|    fps             | 223      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 9        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_29\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 14       |\n",
            "| time/              |          |\n",
            "|    fps             | 226      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 9        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_30\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 20       |\n",
            "| time/              |          |\n",
            "|    fps             | 224      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 9        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_31\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 15.6     |\n",
            "| time/              |          |\n",
            "|    fps             | 224      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 9        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_32\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 13.3     |\n",
            "| time/              |          |\n",
            "|    fps             | 224      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 9        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_33\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 7.47     |\n",
            "| time/              |          |\n",
            "|    fps             | 226      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 9        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_34\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 20       |\n",
            "| time/              |          |\n",
            "|    fps             | 227      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 9        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_35\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 20       |\n",
            "| time/              |          |\n",
            "|    fps             | 225      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 9        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_36\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 14.7     |\n",
            "| time/              |          |\n",
            "|    fps             | 224      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 9        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_37\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 14.7     |\n",
            "| time/              |          |\n",
            "|    fps             | 225      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 9        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_38\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 13.3     |\n",
            "| time/              |          |\n",
            "|    fps             | 224      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 9        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_39\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 13.1     |\n",
            "| time/              |          |\n",
            "|    fps             | 225      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 9        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_40\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 16.4     |\n",
            "| time/              |          |\n",
            "|    fps             | 225      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 9        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_41\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 16.4     |\n",
            "| time/              |          |\n",
            "|    fps             | 223      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 9        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_42\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 14.7     |\n",
            "| time/              |          |\n",
            "|    fps             | 223      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 9        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_43\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 13.1     |\n",
            "| time/              |          |\n",
            "|    fps             | 227      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 8        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_44\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 21.5     |\n",
            "| time/              |          |\n",
            "|    fps             | 226      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 9        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_45\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 15.6     |\n",
            "| time/              |          |\n",
            "|    fps             | 227      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 8        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_46\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 22.2     |\n",
            "| time/              |          |\n",
            "|    fps             | 226      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 9        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_47\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 21.4     |\n",
            "| time/              |          |\n",
            "|    fps             | 224      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 9        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_48\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 8.89     |\n",
            "| time/              |          |\n",
            "|    fps             | 226      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 9        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_49\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 20       |\n",
            "| time/              |          |\n",
            "|    fps             | 225      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 9        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_50\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 22.2     |\n",
            "| time/              |          |\n",
            "|    fps             | 226      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 9        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_51\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2e+03    |\n",
            "|    ep_rew_mean     | 15.6     |\n",
            "| time/              |          |\n",
            "|    fps             | 224      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 9        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n"
          ]
        }
      ],
      "source": [
        "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
        "\n",
        "\n",
        "class JobSchedulingEnv(gym.Env):\n",
        "    def __init__(self, num_jobs=6, job_durations=[2, 3, 5, 6, 2, 3], num_machines=2):\n",
        "        super(JobSchedulingEnv, self).__init__()\n",
        "        self.num_jobs = num_jobs\n",
        "        self.job_durations = job_durations\n",
        "        self.num_machines = num_machines\n",
        "        self.action_space = spaces.MultiDiscrete([num_machines] * num_jobs)\n",
        "        self.observation_space = spaces.MultiDiscrete([num_machines] * num_jobs)\n",
        "        self.max_steps = 2000\n",
        "        self.reset()\n",
        "\n",
        "    def step(self, action):\n",
        "        prev_max_makespan = max([sum(self.job_durations[j] for j in range(self.num_jobs) if self.state[j] == m) for m in range(self.num_machines)])\n",
        "        self.state = action\n",
        "        makespans = [sum(self.job_durations[j] for j in range(self.num_jobs) if self.state[j] == m) for m in range(self.num_machines)]\n",
        "        new_max_makespan = max(makespans)\n",
        "        reward = prev_max_makespan - new_max_makespan\n",
        "        self.current_step += 1\n",
        "        done = self.current_step >= self.max_steps\n",
        "        return self.state, reward, done, {}\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = np.zeros(self.num_jobs, dtype=int)\n",
        "        self.current_makespan = sum(self.job_durations)\n",
        "        self.current_step = 0\n",
        "        return self.state\n",
        "\n",
        "    def render(self, mode='human', close=False):\n",
        "        if close:\n",
        "            return\n",
        "        for m in range(self.num_machines):\n",
        "            jobs_on_machine = [f\"J{i+1}\" for i in range(self.num_jobs) if self.state[i] == m]\n",
        "            makespan = sum(self.job_durations[i] for i in range(self.num_jobs) if self.state[i] == m)\n",
        "            print(f\"Machine {m+1} - Jobs: {', '.join(jobs_on_machine)} | Makespan: {makespan} minutes\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "number_of_epochs = 50\n",
        "timesteps_per_epoch = 2000\n",
        "num_jobs = 5\n",
        "num_machines = 3\n",
        "\n",
        "\n",
        "\n",
        "training_scenarios = [\n",
        "    [random.uniform(1, 12) for _ in range(num_jobs)] for _ in range(number_of_epochs)\n",
        "]\n",
        "\n",
        "initial_job_durations = training_scenarios[0]\n",
        "env = JobSchedulingEnv(num_jobs=num_jobs, job_durations=initial_job_durations, num_machines=num_machines)\n",
        "env = make_vec_env(lambda: env, n_envs=1)\n",
        "\n",
        "model = PPO(\"MlpPolicy\", env, learning_rate=0.00025, n_steps=2048, batch_size=64,\n",
        "            gamma=0.99, gae_lambda=0.95, clip_range=0.2, ent_coef=0.01,\n",
        "            verbose=1, tensorboard_log=\"./ppo_job_scheduling_tensorboard/\")\n",
        "# Evaluation callback for logging performance and progress\n",
        "eval_env = make_vec_env(lambda: JobSchedulingEnv(num_jobs=num_jobs, job_durations=initial_job_durations, num_machines=num_machines), n_envs=1)\n",
        "eval_callback = EvalCallback(eval_env, best_model_save_path='./logs/',\n",
        "                             log_path='./logs/', eval_freq=500,\n",
        "                             deterministic=True, render=False)\n",
        "\n",
        "# Checkpoint callback for saving the model\n",
        "checkpoint_callback = CheckpointCallback(save_freq=1000, save_path='./logs/',\n",
        "                                         name_prefix='rl_model')\n",
        "\n",
        "\n",
        "for epoch, new_job_durations in enumerate(training_scenarios):\n",
        "    env.envs[0].env.job_durations = new_job_durations\n",
        "    model.learn(total_timesteps=timesteps_per_epoch)\n",
        "\n",
        "model_filename = f\"job_scheduling_model_epoch_{num_machines}machines_{num_jobs}jobs\"\n",
        "model.save(model_filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOXs79XH2P1x",
        "outputId": "4c22ccd2-0f30-49f3-9633-17a3db2e5114"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Schedule:\n",
            "Machine 1 - Jobs: J2, J4, J5 | Makespan: 12 minutes\n",
            "Machine 2 - Jobs: J1 | Makespan: 10 minutes\n",
            "Machine 3 - Jobs: J3 | Makespan: 20 minutes\n"
          ]
        }
      ],
      "source": [
        "# Load the model- working testing model\n",
        "loaded_model = PPO.load(\"job_scheduling_model_epoch_3machines_5jobs\")\n",
        "\n",
        "# Create an instance of the environment for testing\n",
        "num_jobs = 5\n",
        "job_durations = [10, 3, 20, 8, 1]  # These should match the training setup\n",
        "num_machines = 3\n",
        "test_env = JobSchedulingEnv(num_jobs=num_jobs, job_durations=job_durations, num_machines=num_machines)\n",
        "\n",
        "# Initialize variables to track the optimal solution\n",
        "optimal_makespan = float('inf')\n",
        "optimal_state = None\n",
        "\n",
        "# Run the model to find the optimal solution\n",
        "obs = test_env.reset()\n",
        "for _ in range(20000):\n",
        "    # Introduce a small probability of random action to allow exploration\n",
        "    if random.random() < 0.05:  # 5% chance of random action\n",
        "        action = test_env.action_space.sample()\n",
        "    else:\n",
        "        action, _states = loaded_model.predict(obs, deterministic=False)\n",
        "\n",
        "    obs, _, dones, _ = test_env.step(action)\n",
        "\n",
        "    # Track the best solution\n",
        "    if test_env.current_makespan < optimal_makespan:\n",
        "        optimal_makespan = test_env.current_makespan\n",
        "        optimal_state = obs.copy()\n",
        "\n",
        "    if dones:\n",
        "        obs = test_env.reset()\n",
        "\n",
        "# Print the optimal solution\n",
        "print(\"Optimal Schedule:\")\n",
        "for m in range(num_machines):\n",
        "    jobs_on_machine = [f\"J{i+1}\" for i in range(num_jobs) if optimal_state[i] == m]\n",
        "    makespan = sum(test_env.job_durations[i] for i in range(num_jobs) if optimal_state[i] == m)\n",
        "    print(f\"Machine {m+1} - Jobs: {', '.join(jobs_on_machine)} | Makespan: {makespan} minutes\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UTcdFdvscrN"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./ppo_job_scheduling_tensorboard/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7N6NpxAGsl5T"
      },
      "outputs": [],
      "source": [
        "# Load the model itioal development\n",
        "loaded_model = PPO.load(\"job_scheduling_model\", env=env)\n",
        "\n",
        "# Create an instance of the original environment for rendering\n",
        "render_env = JobSchedulingEnv(num_jobs=8, job_durations=[2, 1, 4, 3, 5, 2, 6, 3], num_machines=3)\n",
        "\n",
        "# Test the loaded model\n",
        "obs = env.reset()\n",
        "for i in range(200000):\n",
        "    action, _states = loaded_model.predict(obs, deterministic=True)\n",
        "    obs, rewards, dones, info = env.step(action)\n",
        "\n",
        "    # Synchronize the state of the rendering environment\n",
        "    render_env.state = env.get_attr(\"state\")[0]\n",
        "    render_env.current_makespan = env.get_attr(\"current_makespan\")[0]\n",
        "\n",
        "    # Use the render method of the original environment\n",
        "    render_env.render()\n",
        "\n",
        "    if dones:\n",
        "        obs = env.reset()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import random\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "class JobSchedulingEnv(gym.Env):\n",
        "    def __init__(self, num_jobs=6, job_durations=[2, 3, 5, 6, 2, 3], num_machines=2):\n",
        "        super(JobSchedulingEnv, self).__init__()\n",
        "        self.num_jobs = num_jobs\n",
        "        self.job_durations = job_durations\n",
        "        self.num_machines = num_machines\n",
        "        self.action_space = spaces.MultiDiscrete([num_machines] * num_jobs)\n",
        "        self.observation_space = spaces.MultiDiscrete([num_machines] * num_jobs)\n",
        "        self.max_steps = 2000\n",
        "        self.reset()\n",
        "\n",
        "    def step(self, action):\n",
        "        prev_max_makespan = max([sum(self.job_durations[j] for j in range(self.num_jobs) if self.state[j] == m) for m in range(self.num_machines)])\n",
        "        self.state = action\n",
        "        makespans = [sum(self.job_durations[j] for j in range(self.num_jobs) if self.state[j] == m) for m in range(self.num_machines)]\n",
        "        new_max_makespan = max(makespans)\n",
        "        reward = prev_max_makespan - new_max_makespan\n",
        "        self.current_step += 1\n",
        "        done = self.current_step >= self.max_steps\n",
        "        return self.state, reward, done, {}\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = np.zeros(self.num_jobs, dtype=int)\n",
        "        self.current_makespan = sum(self.job_durations)\n",
        "        self.current_step = 0\n",
        "        return self.state\n",
        "\n",
        "    def render(self, mode='human', close=False):\n",
        "        if close:\n",
        "            return\n",
        "        for m in range(self.num_machines):\n",
        "            jobs_on_machine = [f\"J{i+1}\" for i in range(self.num_jobs) if self.state[i] == m]\n",
        "            makespan = sum(self.job_durations[i] for i in range(self.num_jobs) if self.state[i] == m)\n",
        "            print(f\"Machine {m+1} - Jobs: {', '.join(jobs_on_machine)} | Makespan: {makespan} minutes\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "# Initialize your environment parameters\n",
        "number_of_epochs = 100\n",
        "timesteps_per_epoch = 2000\n",
        "num_jobs = 5\n",
        "num_machines = 3\n",
        "\n",
        "# Epsilon-Greedy Parameters\n",
        "epsilon_start = 1.0\n",
        "epsilon_end = 0.01\n",
        "epsilon_decay = 0.995\n",
        "\n",
        "def select_action(model, observation, epsilon, env):\n",
        "    if random.random() < epsilon:\n",
        "        # Generate a random action for each environment in the batch\n",
        "        return [env.action_space.sample() for _ in range(env.num_envs)]\n",
        "    else:\n",
        "        # Predict action using the model for each environment in the batch\n",
        "        return model.predict(observation, deterministic=True)[0]\n",
        "\n",
        "\n",
        "# Generating training scenarios\n",
        "training_scenarios = [\n",
        "    [random.uniform(1, 12) for _ in range(num_jobs)] for _ in range(number_of_epochs)\n",
        "]\n",
        "\n",
        "initial_job_durations = training_scenarios[0]\n",
        "env = JobSchedulingEnv(num_jobs=num_jobs, job_durations=initial_job_durations, num_machines=num_machines)\n",
        "env = make_vec_env(lambda: env, n_envs=1)\n",
        "\n",
        "# Initialize the PPO model\n",
        "model = PPO(\"MlpPolicy\", env, learning_rate=0.0025, n_steps=2048, batch_size=64,\n",
        "            gamma=0.99, gae_lambda=0.95, clip_range=0.2, ent_coef=0.01,\n",
        "            verbose=1, tensorboard_log=\"./ppo_job_scheduling_tensorboard/\")\n",
        "\n",
        "# Training loop with epsilon-greedy exploration\n",
        "epsilon = epsilon_start\n",
        "for epoch in range(number_of_epochs):\n",
        "    obs = env.reset()\n",
        "    for step in range(timesteps_per_epoch):\n",
        "        action = select_action(model, obs, epsilon, env)\n",
        "        obs, rewards, dones, infos = env.step(action)\n",
        "        # ... (additional code for your training step) ...\n",
        "\n",
        "    # Decay epsilon\n",
        "    epsilon = max(epsilon_end, epsilon_decay * epsilon)\n",
        "\n",
        "# Saving the model\n",
        "model_filename = f\"job_scheduling_model_epoch_{num_machines}machines_{num_jobs}jobs\"\n",
        "model.save(model_filename)\n",
        "\n",
        "# ... [Your existing code for loading the model and testing] ...\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmfZFL4ROmiC",
        "outputId": "d5e2b72e-a162-4f8f-ca69-2665123541f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model- working testing model\n",
        "loaded_model = PPO.load(\"job_scheduling_model_epoch_3machines_5jobs\")\n",
        "\n",
        "# Create an instance of the environment for testing\n",
        "num_jobs = 5\n",
        "job_durations = [10, 3, 20, 8, 1]  # These should match the training setup\n",
        "num_machines = 3\n",
        "test_env = JobSchedulingEnv(num_jobs=num_jobs, job_durations=job_durations, num_machines=num_machines)\n",
        "\n",
        "# Initialize variables to track the optimal solution\n",
        "optimal_makespan = float('inf')\n",
        "optimal_state = None\n",
        "\n",
        "# Run the model to find the optimal solution\n",
        "obs = test_env.reset()\n",
        "for _ in range(1000):\n",
        "    # Introduce a small probability of random action to allow exploration\n",
        "    if random.random() < 0.05:  # 5% chance of random action\n",
        "        action = test_env.action_space.sample()\n",
        "    else:\n",
        "        action, _states = loaded_model.predict(obs, deterministic=False)\n",
        "\n",
        "    obs, _, dones, _ = test_env.step(action)\n",
        "\n",
        "    # Track the best solution\n",
        "    if test_env.current_makespan < optimal_makespan:\n",
        "        optimal_makespan = test_env.current_makespan\n",
        "        optimal_state = obs.copy()\n",
        "\n",
        "    if dones:\n",
        "        obs = test_env.reset()\n",
        "\n",
        "# Print the optimal solution\n",
        "print(\"Optimal Schedule:\")\n",
        "for m in range(num_machines):\n",
        "    jobs_on_machine = [f\"J{i+1}\" for i in range(num_jobs) if optimal_state[i] == m]\n",
        "    makespan = sum(test_env.job_durations[i] for i in range(num_jobs) if optimal_state[i] == m)\n",
        "    print(f\"Machine {m+1} - Jobs: {', '.join(jobs_on_machine)} | Makespan: {makespan} minutes\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "8kt6gY-DPvNY",
        "outputId": "5aca145c-5344-46e0-f05b-110c91436c1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'PPO' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e5d3635149aa>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the model- working testing model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloaded_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"job_scheduling_model_epoch_3machines_5jobs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Create an instance of the environment for testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnum_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'PPO' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J-i9hMJ08cXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pulp\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J789lzpquol1",
        "outputId": "1bab98ea-aa87-4d17-ed28-f2eacfa27445"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pulp\n",
            "  Downloading PuLP-2.7.0-py3-none-any.whl (14.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.3/14.3 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pulp\n",
            "Successfully installed pulp-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Parameters\n",
        "num_jobs = 5\n",
        "job_durations = [10, 3, 20, 8, 1]\n",
        "num_machines = 3\n",
        "population_size = 50\n",
        "generations = 100\n",
        "crossover_rate = 0.8\n",
        "mutation_rate = 0.1\n",
        "\n",
        "# Initialize population\n",
        "def initialize_population(population_size, num_jobs, num_machines):\n",
        "    return [[random.randint(0, num_machines - 1) for _ in range(num_jobs)] for _ in range(population_size)]\n",
        "\n",
        "# Calculate makespan\n",
        "def calculate_makespan(chromosome, job_durations, num_machines):\n",
        "    machine_times = [0] * num_machines\n",
        "    for job, machine in enumerate(chromosome):\n",
        "        machine_times[machine] += job_durations[job]\n",
        "    return max(machine_times)\n",
        "\n",
        "# Selection - Tournament selection\n",
        "def tournament_selection(population, fitness, tournament_size=3):\n",
        "    selected = []\n",
        "    for _ in range(len(population)):\n",
        "        tournament = [random.choice(range(len(population))) for _ in range(tournament_size)]\n",
        "        fittest_individual = min(tournament, key=lambda i: fitness[i])\n",
        "        selected.append(population[fittest_individual])\n",
        "    return selected\n",
        "\n",
        "# Crossover - Single point crossover\n",
        "def crossover(parent1, parent2):\n",
        "    if random.random() < crossover_rate:\n",
        "        point = random.randint(1, len(parent1) - 1)\n",
        "        return parent1[:point] + parent2[point:], parent2[:point] + parent1[point:]\n",
        "    else:\n",
        "        return parent1, parent2\n",
        "\n",
        "# Mutation - Randomly change a job's machine assignment\n",
        "def mutate(chromosome, num_machines, mutation_rate):\n",
        "    for i in range(len(chromosome)):\n",
        "        if random.random() < mutation_rate:\n",
        "            chromosome[i] = random.randint(0, num_machines - 1)\n",
        "    return chromosome\n",
        "\n",
        "# Main Genetic Algorithm\n",
        "population = initialize_population(population_size, num_jobs, num_machines)\n",
        "\n",
        "for generation in range(generations):\n",
        "    # Calculate fitness for each individual\n",
        "    fitness = [calculate_makespan(individual, job_durations, num_machines) for individual in population]\n",
        "\n",
        "    # Selection\n",
        "    selected = tournament_selection(population, fitness)\n",
        "\n",
        "    # Crossover\n",
        "    offspring = []\n",
        "    for i in range(0, len(selected), 2):\n",
        "        parent1, parent2 = selected[i], selected[i + 1]\n",
        "        child1, child2 = crossover(parent1, parent2)\n",
        "        offspring.extend([child1, child2])\n",
        "\n",
        "    # Mutation\n",
        "    population = [mutate(individual, num_machines, mutation_rate) for individual in offspring]\n",
        "\n",
        "# Find the best solution\n",
        "best_solution = min(population, key=lambda chrom: calculate_makespan(chrom, job_durations, num_machines))\n",
        "best_makespan = calculate_makespan(best_solution, job_durations, num_machines)\n",
        "\n",
        "print(\"Best Schedule:\", best_solution)\n",
        "print(\"Best Makespan:\", best_makespan)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlznDVTDvDp9",
        "outputId": "2fcefc73-ae56-4f4e-9c49-9c6a0053d136"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Schedule: [1, 2, 0, 2, 1]\n",
            "Best Makespan: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import random\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "import random\n",
        "\n",
        "# Genetic Algorithm Functions\n",
        "def initialize_population(population_size, num_jobs, num_machines):\n",
        "    return [[random.randint(0, num_machines - 1) for _ in range(num_jobs)] for _ in range(population_size)]\n",
        "\n",
        "def calculate_makespan(chromosome, job_durations, num_machines):\n",
        "    machine_times = [0] * num_machines\n",
        "    for job, machine in enumerate(chromosome):\n",
        "        machine_times[machine] += job_durations[job]\n",
        "    return max(machine_times)\n",
        "\n",
        "def tournament_selection(population, fitness, tournament_size=3):\n",
        "    selected = []\n",
        "    for _ in range(len(population)):\n",
        "        tournament = [random.choice(range(len(population))) for _ in range(tournament_size)]\n",
        "        fittest_individual = min(tournament, key=lambda i: fitness[i])\n",
        "        selected.append(population[fittest_individual])\n",
        "    return selected\n",
        "\n",
        "def crossover(parent1, parent2, crossover_rate):\n",
        "    if random.random() < crossover_rate:\n",
        "        point = random.randint(1, len(parent1) - 1)\n",
        "        return parent1[:point] + parent2[point:], parent2[:point] + parent1[point:]\n",
        "    else:\n",
        "        return parent1, parent2\n",
        "\n",
        "def mutate(chromosome, num_machines, mutation_rate):\n",
        "    for i in range(len(chromosome)):\n",
        "        if random.random() < mutation_rate:\n",
        "            chromosome[i] = random.randint(0, num_machines - 1)\n",
        "    return chromosome\n",
        "\n",
        "def run_genetic_algorithm(num_jobs, job_durations, num_machines, population_size, generations, crossover_rate, mutation_rate):\n",
        "    population = initialize_population(population_size, num_jobs, num_machines)\n",
        "\n",
        "    for generation in range(generations):\n",
        "        fitness = [calculate_makespan(individual, job_durations, num_machines) for individual in population]\n",
        "        selected = tournament_selection(population, fitness)\n",
        "        offspring = []\n",
        "        for i in range(0, len(selected), 2):\n",
        "            parent1, parent2 = selected[i], selected[i + 1]\n",
        "            child1, child2 = crossover(parent1, parent2, crossover_rate)\n",
        "            offspring.extend([child1, child2])\n",
        "        population = [mutate(individual, num_machines, mutation_rate) for individual in offspring]\n",
        "\n",
        "    best_solution = min(population, key=lambda chrom: calculate_makespan(chrom, job_durations, num_machines))\n",
        "    best_makespan = calculate_makespan(best_solution, job_durations, num_machines)\n",
        "    print(best_makespan)\n",
        "    return best_makespan\n",
        "\n",
        "\n",
        "\n",
        "class JobSchedulingEnv(gym.Env):\n",
        "    def __init__(self, num_jobs=5, job_durations=[10, 3, 20, 8, 1], num_machines=3, target_makespan=20,tolerance=1):\n",
        "        super(JobSchedulingEnv, self).__init__()\n",
        "        self.num_jobs = num_jobs\n",
        "        self.job_durations = job_durations\n",
        "        self.num_machines = num_machines\n",
        "        self.target_makespan = target_makespan\n",
        "        self.action_space = spaces.Discrete(num_jobs * num_machines)  # New action space\n",
        "        self.observation_space = spaces.Box(low=0, high=num_machines, shape=(num_jobs,), dtype=np.int32)\n",
        "        self.max_steps = 2000\n",
        "        self.tolerance = tolerance\n",
        "        self.state = None\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = np.random.randint(low=0, high=self.num_machines, size=self.num_jobs)  # Random initial state\n",
        "        self.current_step = 0\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        job_index = action // self.num_machines\n",
        "        machine_index = action % self.num_machines\n",
        "        self.state[job_index] = machine_index\n",
        "        current_makespan = max([sum(self.job_durations[j] for j in range(self.num_jobs) if self.state[j] == m) for m in range(self.num_machines)])\n",
        "        reward = -abs(current_makespan - self.target_makespan)  # Negative absolute difference as reward\n",
        "        self.current_step += 1\n",
        "        done = abs(current_makespan - self.target_makespan) <= self.tolerance\n",
        "        return self.state, reward, done, {}\n",
        "\n",
        "\n",
        "    def render(self, mode='human', close=False):\n",
        "        if close:\n",
        "            return\n",
        "        for m in range(self.num_machines):\n",
        "            jobs_on_machine = [f\"J{i+1}\" for i in range(self.num_jobs) if self.state[i] == m]\n",
        "            makespan = sum(self.job_durations[i] for i in range(self.num_jobs) if self.state[i] == m)\n",
        "            print(f\"Machine {m+1} - Jobs: {', '.join(jobs_on_machine)} | Makespan: {makespan} minutes\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "# Training Parameters\n",
        "number_of_epochs = 3\n",
        "timesteps_per_epoch = 40000\n",
        "num_jobs = 5\n",
        "num_machines = 3\n",
        "population_size = 50\n",
        "generations = 100\n",
        "crossover_rate = 0.8\n",
        "mutation_rate = 0.1\n",
        "tolerance = 5\n",
        "\n",
        "# Initialize the PPO model\n",
        "dummy_env = JobSchedulingEnv(num_jobs=num_jobs, job_durations=[1] * num_jobs, num_machines=num_machines, target_makespan=1,tolerance=tolerance)\n",
        "model = PPO(\"MlpPolicy\", dummy_env, learning_rate=0.00025, n_steps=2000, batch_size=64, gamma=0.99, gae_lambda=0.95, clip_range=0.2, ent_coef=0.01, verbose=1, tensorboard_log=\"./ppo_job_scheduling_tensorboard/\")\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(number_of_epochs):\n",
        "    # Generate random job durations for each epoch\n",
        "    job_durations = [random.randint(1, 5) for _ in range(num_jobs)]\n",
        "\n",
        "    # Run the Genetic Algorithm to find the target makespan\n",
        "    target_makespan = run_genetic_algorithm(num_jobs, job_durations, num_machines, population_size, generations, crossover_rate, mutation_rate)\n",
        "\n",
        "\n",
        "    # Create the real environment with new parameters\n",
        "    real_env = JobSchedulingEnv(num_jobs=num_jobs, job_durations=job_durations, num_machines=num_machines, target_makespan=target_makespan,tolerance=tolerance)\n",
        "    real_env = make_vec_env(lambda: real_env, n_envs=1)\n",
        "\n",
        "    # Update the model's environment\n",
        "    model.set_env(real_env)\n",
        "\n",
        "    # Train the model\n",
        "    model.learn(total_timesteps=timesteps_per_epoch)\n",
        "\n",
        "\n",
        "# Saving the model\n",
        "model.save(\"job_scheduling_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3f3id_58eUx",
        "outputId": "18c29534-95c7-4e11-d00b-871080d1afc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "3\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_290\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/ppo/ppo.py:155: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 2000`, after every 31 untruncated mini-batches, there will be a truncated mini-batch of size 16\n",
            "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
            "Info: (n_steps=2000 and n_envs=1)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1.04     |\n",
            "|    ep_rew_mean     | -2.58    |\n",
            "| time/              |          |\n",
            "|    fps             | 684      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 2        |\n",
            "|    total_timesteps | 2000     |\n",
            "---------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.02        |\n",
            "|    ep_rew_mean          | -2.41       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 553         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 7           |\n",
            "|    total_timesteps      | 4000        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018658869 |\n",
            "|    clip_fraction        | 0.149       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.7        |\n",
            "|    explained_variance   | -0.0262     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.536       |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0304     |\n",
            "|    value_loss           | 4.86        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.02        |\n",
            "|    ep_rew_mean          | -2.4        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 503         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 11          |\n",
            "|    total_timesteps      | 6000        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021347404 |\n",
            "|    clip_fraction        | 0.28        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.64       |\n",
            "|    explained_variance   | -0.0314     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.853       |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0446     |\n",
            "|    value_loss           | 2.46        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.03        |\n",
            "|    ep_rew_mean          | -2.29       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 492         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 16          |\n",
            "|    total_timesteps      | 8000        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021778788 |\n",
            "|    clip_fraction        | 0.298       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.56       |\n",
            "|    explained_variance   | 0.0757      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.569       |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0519     |\n",
            "|    value_loss           | 2.37        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1.01       |\n",
            "|    ep_rew_mean          | -2.07      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 486        |\n",
            "|    iterations           | 5          |\n",
            "|    time_elapsed         | 20         |\n",
            "|    total_timesteps      | 10000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01946235 |\n",
            "|    clip_fraction        | 0.365      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.44      |\n",
            "|    explained_variance   | 0.122      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 0.543      |\n",
            "|    n_updates            | 40         |\n",
            "|    policy_gradient_loss | -0.0646    |\n",
            "|    value_loss           | 1.99       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1           |\n",
            "|    ep_rew_mean          | -1.98       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 474         |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 25          |\n",
            "|    total_timesteps      | 12000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021550588 |\n",
            "|    clip_fraction        | 0.39        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.32       |\n",
            "|    explained_variance   | 0.147       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.495       |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.0745     |\n",
            "|    value_loss           | 1.56        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.01        |\n",
            "|    ep_rew_mean          | -1.93       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 473         |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 29          |\n",
            "|    total_timesteps      | 14000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028465461 |\n",
            "|    clip_fraction        | 0.401       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.16       |\n",
            "|    explained_variance   | 0.13        |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.183       |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.0762     |\n",
            "|    value_loss           | 1.15        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1           |\n",
            "|    ep_rew_mean          | -1.63       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 468         |\n",
            "|    iterations           | 8           |\n",
            "|    time_elapsed         | 34          |\n",
            "|    total_timesteps      | 16000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020976381 |\n",
            "|    clip_fraction        | 0.261       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.99       |\n",
            "|    explained_variance   | 0.0626      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.327       |\n",
            "|    n_updates            | 70          |\n",
            "|    policy_gradient_loss | -0.0611     |\n",
            "|    value_loss           | 0.928       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1           |\n",
            "|    ep_rew_mean          | -1.5        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 465         |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 38          |\n",
            "|    total_timesteps      | 18000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018754754 |\n",
            "|    clip_fraction        | 0.222       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.85       |\n",
            "|    explained_variance   | 0.0648      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.261       |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.0523     |\n",
            "|    value_loss           | 0.8         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1           |\n",
            "|    ep_rew_mean          | -1.49       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 465         |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 43          |\n",
            "|    total_timesteps      | 20000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016115401 |\n",
            "|    clip_fraction        | 0.213       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.71       |\n",
            "|    explained_variance   | 0.105       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.198       |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | -0.0449     |\n",
            "|    value_loss           | 0.614       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1           |\n",
            "|    ep_rew_mean          | -1.36       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 461         |\n",
            "|    iterations           | 11          |\n",
            "|    time_elapsed         | 47          |\n",
            "|    total_timesteps      | 22000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015000742 |\n",
            "|    clip_fraction        | 0.145       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.63       |\n",
            "|    explained_variance   | 0.0638      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.138       |\n",
            "|    n_updates            | 100         |\n",
            "|    policy_gradient_loss | -0.0382     |\n",
            "|    value_loss           | 0.688       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1           |\n",
            "|    ep_rew_mean          | -1.43       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 461         |\n",
            "|    iterations           | 12          |\n",
            "|    time_elapsed         | 52          |\n",
            "|    total_timesteps      | 24000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013525712 |\n",
            "|    clip_fraction        | 0.143       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.54       |\n",
            "|    explained_variance   | 0.101       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.206       |\n",
            "|    n_updates            | 110         |\n",
            "|    policy_gradient_loss | -0.0371     |\n",
            "|    value_loss           | 0.566       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1           |\n",
            "|    ep_rew_mean          | -1.43       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 460         |\n",
            "|    iterations           | 13          |\n",
            "|    time_elapsed         | 56          |\n",
            "|    total_timesteps      | 26000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015544775 |\n",
            "|    clip_fraction        | 0.165       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.43       |\n",
            "|    explained_variance   | 0.0905      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.168       |\n",
            "|    n_updates            | 120         |\n",
            "|    policy_gradient_loss | -0.0383     |\n",
            "|    value_loss           | 0.523       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1           |\n",
            "|    ep_rew_mean          | -1.3        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 457         |\n",
            "|    iterations           | 14          |\n",
            "|    time_elapsed         | 61          |\n",
            "|    total_timesteps      | 28000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014967583 |\n",
            "|    clip_fraction        | 0.155       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.38       |\n",
            "|    explained_variance   | 0.118       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.211       |\n",
            "|    n_updates            | 130         |\n",
            "|    policy_gradient_loss | -0.0362     |\n",
            "|    value_loss           | 0.516       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1           |\n",
            "|    ep_rew_mean          | -1.22       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 457         |\n",
            "|    iterations           | 15          |\n",
            "|    time_elapsed         | 65          |\n",
            "|    total_timesteps      | 30000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013354813 |\n",
            "|    clip_fraction        | 0.112       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.29       |\n",
            "|    explained_variance   | 0.0782      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.228       |\n",
            "|    n_updates            | 140         |\n",
            "|    policy_gradient_loss | -0.0335     |\n",
            "|    value_loss           | 0.499       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1           |\n",
            "|    ep_rew_mean          | -1.45       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 455         |\n",
            "|    iterations           | 16          |\n",
            "|    time_elapsed         | 70          |\n",
            "|    total_timesteps      | 32000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010692762 |\n",
            "|    clip_fraction        | 0.102       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.22       |\n",
            "|    explained_variance   | 0.0932      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.278       |\n",
            "|    n_updates            | 150         |\n",
            "|    policy_gradient_loss | -0.0306     |\n",
            "|    value_loss           | 0.484       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1           |\n",
            "|    ep_rew_mean          | -1.29       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 455         |\n",
            "|    iterations           | 17          |\n",
            "|    time_elapsed         | 74          |\n",
            "|    total_timesteps      | 34000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014578375 |\n",
            "|    clip_fraction        | 0.0936      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.17       |\n",
            "|    explained_variance   | 0.11        |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.431       |\n",
            "|    n_updates            | 160         |\n",
            "|    policy_gradient_loss | -0.0256     |\n",
            "|    value_loss           | 0.441       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1           |\n",
            "|    ep_rew_mean          | -1.38       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 455         |\n",
            "|    iterations           | 18          |\n",
            "|    time_elapsed         | 78          |\n",
            "|    total_timesteps      | 36000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011278259 |\n",
            "|    clip_fraction        | 0.13        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.13       |\n",
            "|    explained_variance   | 0.129       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.131       |\n",
            "|    n_updates            | 170         |\n",
            "|    policy_gradient_loss | -0.0309     |\n",
            "|    value_loss           | 0.463       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1           |\n",
            "|    ep_rew_mean          | -1.19       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 453         |\n",
            "|    iterations           | 19          |\n",
            "|    time_elapsed         | 83          |\n",
            "|    total_timesteps      | 38000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009060815 |\n",
            "|    clip_fraction        | 0.0945      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.06       |\n",
            "|    explained_variance   | 0.11        |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.136       |\n",
            "|    n_updates            | 180         |\n",
            "|    policy_gradient_loss | -0.0258     |\n",
            "|    value_loss           | 0.442       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1           |\n",
            "|    ep_rew_mean          | -1.34       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 453         |\n",
            "|    iterations           | 20          |\n",
            "|    time_elapsed         | 88          |\n",
            "|    total_timesteps      | 40000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008264336 |\n",
            "|    clip_fraction        | 0.0913      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1          |\n",
            "|    explained_variance   | 0.178       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.158       |\n",
            "|    n_updates            | 190         |\n",
            "|    policy_gradient_loss | -0.0216     |\n",
            "|    value_loss           | 0.399       |\n",
            "-----------------------------------------\n",
            "7\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_291\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1.03     |\n",
            "|    ep_rew_mean     | -2.31    |\n",
            "| time/              |          |\n",
            "|    fps             | 664      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 3        |\n",
            "|    total_timesteps | 2000     |\n",
            "---------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.08        |\n",
            "|    ep_rew_mean          | -2.75       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 527         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 7           |\n",
            "|    total_timesteps      | 4000        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010680847 |\n",
            "|    clip_fraction        | 0.0947      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.934      |\n",
            "|    explained_variance   | 0.0364      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.87        |\n",
            "|    n_updates            | 210         |\n",
            "|    policy_gradient_loss | -0.019      |\n",
            "|    value_loss           | 5.42        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1.01       |\n",
            "|    ep_rew_mean          | -2.13      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 507        |\n",
            "|    iterations           | 3          |\n",
            "|    time_elapsed         | 11         |\n",
            "|    total_timesteps      | 6000       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00815574 |\n",
            "|    clip_fraction        | 0.0967     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.921     |\n",
            "|    explained_variance   | 0.121      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 0.665      |\n",
            "|    n_updates            | 220        |\n",
            "|    policy_gradient_loss | -0.0183    |\n",
            "|    value_loss           | 4.63       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.03        |\n",
            "|    ep_rew_mean          | -2.48       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 485         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 16          |\n",
            "|    total_timesteps      | 8000        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010172943 |\n",
            "|    clip_fraction        | 0.102       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.898      |\n",
            "|    explained_variance   | 0.208       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.895       |\n",
            "|    n_updates            | 230         |\n",
            "|    policy_gradient_loss | -0.0221     |\n",
            "|    value_loss           | 4.27        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.01        |\n",
            "|    ep_rew_mean          | -2.01       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 482         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 20          |\n",
            "|    total_timesteps      | 10000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011878994 |\n",
            "|    clip_fraction        | 0.138       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.914      |\n",
            "|    explained_variance   | 0.271       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.992       |\n",
            "|    n_updates            | 240         |\n",
            "|    policy_gradient_loss | -0.0256     |\n",
            "|    value_loss           | 3.81        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1           |\n",
            "|    ep_rew_mean          | -2.1        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 478         |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 25          |\n",
            "|    total_timesteps      | 12000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009134321 |\n",
            "|    clip_fraction        | 0.104       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.866      |\n",
            "|    explained_variance   | 0.288       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.91        |\n",
            "|    n_updates            | 250         |\n",
            "|    policy_gradient_loss | -0.0221     |\n",
            "|    value_loss           | 2.96        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1           |\n",
            "|    ep_rew_mean          | -2.01       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 469         |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 29          |\n",
            "|    total_timesteps      | 14000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011752756 |\n",
            "|    clip_fraction        | 0.122       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.864      |\n",
            "|    explained_variance   | 0.294       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.14        |\n",
            "|    n_updates            | 260         |\n",
            "|    policy_gradient_loss | -0.0243     |\n",
            "|    value_loss           | 2.87        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.01        |\n",
            "|    ep_rew_mean          | -1.9        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 469         |\n",
            "|    iterations           | 8           |\n",
            "|    time_elapsed         | 34          |\n",
            "|    total_timesteps      | 16000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008926092 |\n",
            "|    clip_fraction        | 0.0964      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.852      |\n",
            "|    explained_variance   | 0.357       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.354       |\n",
            "|    n_updates            | 270         |\n",
            "|    policy_gradient_loss | -0.0218     |\n",
            "|    value_loss           | 2.35        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1           |\n",
            "|    ep_rew_mean          | -1.97       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 467         |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 38          |\n",
            "|    total_timesteps      | 18000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009041598 |\n",
            "|    clip_fraction        | 0.0971      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.842      |\n",
            "|    explained_variance   | 0.342       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.87        |\n",
            "|    n_updates            | 280         |\n",
            "|    policy_gradient_loss | -0.0203     |\n",
            "|    value_loss           | 2.14        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1           |\n",
            "|    ep_rew_mean          | -1.59       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 464         |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 43          |\n",
            "|    total_timesteps      | 20000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010144679 |\n",
            "|    clip_fraction        | 0.118       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.831      |\n",
            "|    explained_variance   | 0.45        |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.06        |\n",
            "|    n_updates            | 290         |\n",
            "|    policy_gradient_loss | -0.0216     |\n",
            "|    value_loss           | 1.8         |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1.01       |\n",
            "|    ep_rew_mean          | -2.06      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 464        |\n",
            "|    iterations           | 11         |\n",
            "|    time_elapsed         | 47         |\n",
            "|    total_timesteps      | 22000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00745301 |\n",
            "|    clip_fraction        | 0.104      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.829     |\n",
            "|    explained_variance   | 0.424      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 1.2        |\n",
            "|    n_updates            | 300        |\n",
            "|    policy_gradient_loss | -0.0233    |\n",
            "|    value_loss           | 1.85       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.04        |\n",
            "|    ep_rew_mean          | -2.17       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 459         |\n",
            "|    iterations           | 12          |\n",
            "|    time_elapsed         | 52          |\n",
            "|    total_timesteps      | 24000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006748994 |\n",
            "|    clip_fraction        | 0.09        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.809      |\n",
            "|    explained_variance   | 0.458       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.254       |\n",
            "|    n_updates            | 310         |\n",
            "|    policy_gradient_loss | -0.0245     |\n",
            "|    value_loss           | 1.62        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.03        |\n",
            "|    ep_rew_mean          | -2.26       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 460         |\n",
            "|    iterations           | 13          |\n",
            "|    time_elapsed         | 56          |\n",
            "|    total_timesteps      | 26000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008977774 |\n",
            "|    clip_fraction        | 0.135       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.807      |\n",
            "|    explained_variance   | 0.449       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.316       |\n",
            "|    n_updates            | 320         |\n",
            "|    policy_gradient_loss | -0.0283     |\n",
            "|    value_loss           | 1.73        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.04         |\n",
            "|    ep_rew_mean          | -2.11        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 460          |\n",
            "|    iterations           | 14           |\n",
            "|    time_elapsed         | 60           |\n",
            "|    total_timesteps      | 28000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0083325375 |\n",
            "|    clip_fraction        | 0.0859       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.732       |\n",
            "|    explained_variance   | 0.508        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.586        |\n",
            "|    n_updates            | 330          |\n",
            "|    policy_gradient_loss | -0.0241      |\n",
            "|    value_loss           | 1.49         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1            |\n",
            "|    ep_rew_mean          | -1.74        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 458          |\n",
            "|    iterations           | 15           |\n",
            "|    time_elapsed         | 65           |\n",
            "|    total_timesteps      | 30000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0094406605 |\n",
            "|    clip_fraction        | 0.109        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.726       |\n",
            "|    explained_variance   | 0.536        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 1.17         |\n",
            "|    n_updates            | 340          |\n",
            "|    policy_gradient_loss | -0.0258      |\n",
            "|    value_loss           | 1.25         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.01        |\n",
            "|    ep_rew_mean          | -1.87       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 458         |\n",
            "|    iterations           | 16          |\n",
            "|    time_elapsed         | 69          |\n",
            "|    total_timesteps      | 32000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008126431 |\n",
            "|    clip_fraction        | 0.0842      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.682      |\n",
            "|    explained_variance   | 0.579       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.133       |\n",
            "|    n_updates            | 350         |\n",
            "|    policy_gradient_loss | -0.0227     |\n",
            "|    value_loss           | 1.16        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.01         |\n",
            "|    ep_rew_mean          | -1.87        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 457          |\n",
            "|    iterations           | 17           |\n",
            "|    time_elapsed         | 74           |\n",
            "|    total_timesteps      | 34000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0076401755 |\n",
            "|    clip_fraction        | 0.0826       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.638       |\n",
            "|    explained_variance   | 0.617        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.664        |\n",
            "|    n_updates            | 360          |\n",
            "|    policy_gradient_loss | -0.0202      |\n",
            "|    value_loss           | 1.09         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.03         |\n",
            "|    ep_rew_mean          | -1.96        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 457          |\n",
            "|    iterations           | 18           |\n",
            "|    time_elapsed         | 78           |\n",
            "|    total_timesteps      | 36000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0066925795 |\n",
            "|    clip_fraction        | 0.0892       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.633       |\n",
            "|    explained_variance   | 0.648        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.64         |\n",
            "|    n_updates            | 370          |\n",
            "|    policy_gradient_loss | -0.0218      |\n",
            "|    value_loss           | 0.991        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.03        |\n",
            "|    ep_rew_mean          | -1.82       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 457         |\n",
            "|    iterations           | 19          |\n",
            "|    time_elapsed         | 83          |\n",
            "|    total_timesteps      | 38000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008884434 |\n",
            "|    clip_fraction        | 0.0955      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.609      |\n",
            "|    explained_variance   | 0.646       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.415       |\n",
            "|    n_updates            | 380         |\n",
            "|    policy_gradient_loss | -0.0227     |\n",
            "|    value_loss           | 0.936       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.01        |\n",
            "|    ep_rew_mean          | -1.7        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 455         |\n",
            "|    iterations           | 20          |\n",
            "|    time_elapsed         | 87          |\n",
            "|    total_timesteps      | 40000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014576204 |\n",
            "|    clip_fraction        | 0.108       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.596      |\n",
            "|    explained_variance   | 0.648       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.0926      |\n",
            "|    n_updates            | 390         |\n",
            "|    policy_gradient_loss | -0.0234     |\n",
            "|    value_loss           | 0.863       |\n",
            "-----------------------------------------\n",
            "8\n",
            "Logging to ./ppo_job_scheduling_tensorboard/PPO_292\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1.05     |\n",
            "|    ep_rew_mean     | -3.09    |\n",
            "| time/              |          |\n",
            "|    fps             | 692      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 2        |\n",
            "|    total_timesteps | 2000     |\n",
            "---------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.08        |\n",
            "|    ep_rew_mean          | -2.82       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 558         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 7           |\n",
            "|    total_timesteps      | 4000        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006262525 |\n",
            "|    clip_fraction        | 0.062       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.565      |\n",
            "|    explained_variance   | 0.428       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.46        |\n",
            "|    n_updates            | 410         |\n",
            "|    policy_gradient_loss | -0.0182     |\n",
            "|    value_loss           | 3.5         |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1.04       |\n",
            "|    ep_rew_mean          | -2.84      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 506        |\n",
            "|    iterations           | 3          |\n",
            "|    time_elapsed         | 11         |\n",
            "|    total_timesteps      | 6000       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00887173 |\n",
            "|    clip_fraction        | 0.092      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.587     |\n",
            "|    explained_variance   | 0.568      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 0.619      |\n",
            "|    n_updates            | 420        |\n",
            "|    policy_gradient_loss | -0.0193    |\n",
            "|    value_loss           | 2.28       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.02        |\n",
            "|    ep_rew_mean          | -2.35       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 495         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 16          |\n",
            "|    total_timesteps      | 8000        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011628484 |\n",
            "|    clip_fraction        | 0.0972      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.57       |\n",
            "|    explained_variance   | 0.656       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.444       |\n",
            "|    n_updates            | 430         |\n",
            "|    policy_gradient_loss | -0.0253     |\n",
            "|    value_loss           | 2.36        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.01        |\n",
            "|    ep_rew_mean          | -2.45       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 483         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 20          |\n",
            "|    total_timesteps      | 10000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009056151 |\n",
            "|    clip_fraction        | 0.0908      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.568      |\n",
            "|    explained_variance   | 0.694       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.414       |\n",
            "|    n_updates            | 440         |\n",
            "|    policy_gradient_loss | -0.0251     |\n",
            "|    value_loss           | 1.47        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.08        |\n",
            "|    ep_rew_mean          | -3.1        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 479         |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 25          |\n",
            "|    total_timesteps      | 12000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008488921 |\n",
            "|    clip_fraction        | 0.0963      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.599      |\n",
            "|    explained_variance   | 0.698       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.41        |\n",
            "|    n_updates            | 450         |\n",
            "|    policy_gradient_loss | -0.0279     |\n",
            "|    value_loss           | 1.7         |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.03         |\n",
            "|    ep_rew_mean          | -2.56        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 477          |\n",
            "|    iterations           | 7            |\n",
            "|    time_elapsed         | 29           |\n",
            "|    total_timesteps      | 14000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0091473125 |\n",
            "|    clip_fraction        | 0.0952       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.599       |\n",
            "|    explained_variance   | 0.659        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.93         |\n",
            "|    n_updates            | 460          |\n",
            "|    policy_gradient_loss | -0.0293      |\n",
            "|    value_loss           | 1.83         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.06        |\n",
            "|    ep_rew_mean          | -2.79       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 469         |\n",
            "|    iterations           | 8           |\n",
            "|    time_elapsed         | 34          |\n",
            "|    total_timesteps      | 16000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012551058 |\n",
            "|    clip_fraction        | 0.13        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.59       |\n",
            "|    explained_variance   | 0.708       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.932       |\n",
            "|    n_updates            | 470         |\n",
            "|    policy_gradient_loss | -0.0333     |\n",
            "|    value_loss           | 1.51        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.04        |\n",
            "|    ep_rew_mean          | -2.74       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 470         |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 38          |\n",
            "|    total_timesteps      | 18000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010959093 |\n",
            "|    clip_fraction        | 0.101       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.585      |\n",
            "|    explained_variance   | 0.707       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.745       |\n",
            "|    n_updates            | 480         |\n",
            "|    policy_gradient_loss | -0.0261     |\n",
            "|    value_loss           | 1.52        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.02        |\n",
            "|    ep_rew_mean          | -2.46       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 469         |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 42          |\n",
            "|    total_timesteps      | 20000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013288228 |\n",
            "|    clip_fraction        | 0.13        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.608      |\n",
            "|    explained_variance   | 0.718       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.24        |\n",
            "|    n_updates            | 490         |\n",
            "|    policy_gradient_loss | -0.033      |\n",
            "|    value_loss           | 1.4         |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.03         |\n",
            "|    ep_rew_mean          | -2.23        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 465          |\n",
            "|    iterations           | 11           |\n",
            "|    time_elapsed         | 47           |\n",
            "|    total_timesteps      | 22000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0113274045 |\n",
            "|    clip_fraction        | 0.11         |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.607       |\n",
            "|    explained_variance   | 0.756        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.658        |\n",
            "|    n_updates            | 500          |\n",
            "|    policy_gradient_loss | -0.03        |\n",
            "|    value_loss           | 1.25         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.03        |\n",
            "|    ep_rew_mean          | -2.43       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 465         |\n",
            "|    iterations           | 12          |\n",
            "|    time_elapsed         | 51          |\n",
            "|    total_timesteps      | 24000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010241794 |\n",
            "|    clip_fraction        | 0.128       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.61       |\n",
            "|    explained_variance   | 0.701       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.29        |\n",
            "|    n_updates            | 510         |\n",
            "|    policy_gradient_loss | -0.0337     |\n",
            "|    value_loss           | 1.37        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.03        |\n",
            "|    ep_rew_mean          | -2.37       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 461         |\n",
            "|    iterations           | 13          |\n",
            "|    time_elapsed         | 56          |\n",
            "|    total_timesteps      | 26000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013626059 |\n",
            "|    clip_fraction        | 0.127       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.579      |\n",
            "|    explained_variance   | 0.682       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.699       |\n",
            "|    n_updates            | 520         |\n",
            "|    policy_gradient_loss | -0.0339     |\n",
            "|    value_loss           | 1.64        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1           |\n",
            "|    ep_rew_mean          | -2.14       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 460         |\n",
            "|    iterations           | 14          |\n",
            "|    time_elapsed         | 60          |\n",
            "|    total_timesteps      | 28000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010502832 |\n",
            "|    clip_fraction        | 0.108       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.569      |\n",
            "|    explained_variance   | 0.71        |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.141       |\n",
            "|    n_updates            | 530         |\n",
            "|    policy_gradient_loss | -0.03       |\n",
            "|    value_loss           | 1.52        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.04        |\n",
            "|    ep_rew_mean          | -2.54       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 461         |\n",
            "|    iterations           | 15          |\n",
            "|    time_elapsed         | 65          |\n",
            "|    total_timesteps      | 30000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009579731 |\n",
            "|    clip_fraction        | 0.107       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.583      |\n",
            "|    explained_variance   | 0.744       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.125       |\n",
            "|    n_updates            | 540         |\n",
            "|    policy_gradient_loss | -0.0325     |\n",
            "|    value_loss           | 1.13        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.01        |\n",
            "|    ep_rew_mean          | -2.15       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 458         |\n",
            "|    iterations           | 16          |\n",
            "|    time_elapsed         | 69          |\n",
            "|    total_timesteps      | 32000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007946995 |\n",
            "|    clip_fraction        | 0.0918      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.566      |\n",
            "|    explained_variance   | 0.736       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.224       |\n",
            "|    n_updates            | 550         |\n",
            "|    policy_gradient_loss | -0.0323     |\n",
            "|    value_loss           | 1.19        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.01        |\n",
            "|    ep_rew_mean          | -1.79       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 458         |\n",
            "|    iterations           | 17          |\n",
            "|    time_elapsed         | 74          |\n",
            "|    total_timesteps      | 34000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008344554 |\n",
            "|    clip_fraction        | 0.0919      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.548      |\n",
            "|    explained_variance   | 0.75        |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.306       |\n",
            "|    n_updates            | 560         |\n",
            "|    policy_gradient_loss | -0.0283     |\n",
            "|    value_loss           | 1.22        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1           |\n",
            "|    ep_rew_mean          | -1.87       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 459         |\n",
            "|    iterations           | 18          |\n",
            "|    time_elapsed         | 78          |\n",
            "|    total_timesteps      | 36000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009061569 |\n",
            "|    clip_fraction        | 0.0885      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.536      |\n",
            "|    explained_variance   | 0.726       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.03        |\n",
            "|    n_updates            | 570         |\n",
            "|    policy_gradient_loss | -0.0279     |\n",
            "|    value_loss           | 1.3         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.02        |\n",
            "|    ep_rew_mean          | -1.9        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 456         |\n",
            "|    iterations           | 19          |\n",
            "|    time_elapsed         | 83          |\n",
            "|    total_timesteps      | 38000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007086073 |\n",
            "|    clip_fraction        | 0.0761      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.533      |\n",
            "|    explained_variance   | 0.753       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.175       |\n",
            "|    n_updates            | 580         |\n",
            "|    policy_gradient_loss | -0.0283     |\n",
            "|    value_loss           | 1.17        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.02         |\n",
            "|    ep_rew_mean          | -1.96        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 457          |\n",
            "|    iterations           | 20           |\n",
            "|    time_elapsed         | 87           |\n",
            "|    total_timesteps      | 40000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0077676857 |\n",
            "|    clip_fraction        | 0.0776       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.522       |\n",
            "|    explained_variance   | 0.76         |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 0.254        |\n",
            "|    n_updates            | 590          |\n",
            "|    policy_gradient_loss | -0.0268      |\n",
            "|    value_loss           | 1.18         |\n",
            "------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import PPO\n",
        "\n",
        "# Load the trained model\n",
        "model = PPO.load(\"job_scheduling_model\")\n",
        "\n",
        "# Test job durations and environment setup\n",
        "test_job_durations = [1, 2, 3, 5, 1]  # Example job durations\n",
        "num_jobs = len(test_job_durations)\n",
        "num_machines = 3\n",
        "test_env = JobSchedulingEnv(num_jobs=num_jobs, job_durations=test_job_durations, num_machines=num_machines, target_makespan=20, tolerance=1)\n",
        "\n",
        "# Run the model in the test environment\n",
        "obs = test_env.reset()\n",
        "done = False\n",
        "max_iterations = 30000  # Prevent infinite loop\n",
        "iteration = 0\n",
        "\n",
        "while not done and iteration < max_iterations:\n",
        "    action, _states = model.predict(obs, deterministic=True)\n",
        "    obs, rewards, done, info = test_env.step(action)\n",
        "    iteration += 1\n",
        "\n",
        "\n",
        "# Check if loop exited due to reaching max iterations\n",
        "if iteration >= max_iterations:\n",
        "    print(\"Reached maximum iterations without fulfilling termination conditions.\")\n",
        "\n",
        "# Print the schedule\n",
        "def print_schedule(env):\n",
        "    print(\"\\nOptimal Schedule:\")\n",
        "    for m in range(env.num_machines):\n",
        "        jobs_on_machine = [f\"J{i+1}\" for i in range(env.num_jobs) if env.state[i] == m]\n",
        "        makespan = sum(env.job_durations[i] for i in range(env.num_jobs) if env.state[i] == m)\n",
        "        print(f\"Machine {m+1} - Jobs: {', '.join(jobs_on_machine)} | Makespan: {makespan} minutes\")\n",
        "\n",
        "print_schedule(test_env)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5qUVb8GA_ng",
        "outputId": "a8f68403-9b97-4286-f819-3ea329f9a0b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reached maximum iterations without fulfilling termination conditions.\n",
            "\n",
            "Optimal Schedule:\n",
            "Machine 1 - Jobs: J5 | Makespan: 1 minutes\n",
            "Machine 2 - Jobs: J1, J3 | Makespan: 4 minutes\n",
            "Machine 3 - Jobs: J2, J4 | Makespan: 7 minutes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Parameters\n",
        "num_jobs = 10\n",
        "job_durations = [10, 3, 20, 8, 1,10, 3, 20, 8, 1]\n",
        "num_machines = 5\n",
        "population_size = 50\n",
        "generations = 100\n",
        "crossover_rate = 0.8\n",
        "mutation_rate = 0.1\n",
        "\n",
        "# Initialize population\n",
        "def initialize_population(population_size, num_jobs, num_machines):\n",
        "    return [[random.randint(0, num_machines - 1) for _ in range(num_jobs)] for _ in range(population_size)]\n",
        "\n",
        "# Calculate makespan and balance\n",
        "def calculate_makespan_and_balance(chromosome, job_durations, num_machines):\n",
        "    machine_times = [0] * num_machines\n",
        "    for job, machine in enumerate(chromosome):\n",
        "        machine_times[machine] += job_durations[job]\n",
        "    max_makespan = max(machine_times)\n",
        "    balance_penalty = sum([(max_makespan - time)**2 for time in machine_times])  # Penalize unbalanced schedules\n",
        "    return max_makespan + balance_penalty\n",
        "\n",
        "# Tournament selection\n",
        "def tournament_selection(population, fitness, tournament_size=3):\n",
        "    selected = []\n",
        "    for _ in range(len(population)):\n",
        "        tournament = [random.choice(range(len(population))) for _ in range(tournament_size)]\n",
        "        fittest_individual = min(tournament, key=lambda i: fitness[i])\n",
        "        selected.append(population[fittest_individual])\n",
        "    return selected\n",
        "\n",
        "# Crossover - Single point crossover\n",
        "def crossover(parent1, parent2):\n",
        "    if random.random() < crossover_rate:\n",
        "        point = random.randint(1, len(parent1) - 1)\n",
        "        return parent1[:point] + parent2[point:], parent2[:point] + parent1[point:]\n",
        "    else:\n",
        "        return parent1, parent2\n",
        "\n",
        "# Mutation - Randomly change a job's machine assignment\n",
        "def mutate(chromosome, num_machines, mutation_rate):\n",
        "    for i in range(len(chromosome)):\n",
        "        if random.random() < mutation_rate:\n",
        "            chromosome[i] = random.randint(0, num_machines - 1)\n",
        "    return chromosome\n",
        "\n",
        "# Function to create a readable schedule from the chromosome\n",
        "def create_schedule(chromosome, job_durations):\n",
        "    schedule = {machine: [] for machine in range(num_machines)}\n",
        "    for job, machine in enumerate(chromosome):\n",
        "        schedule[machine].append((f\"J{job+1}\", job_durations[job]))\n",
        "    return schedule\n",
        "\n",
        "# Main Genetic Algorithm\n",
        "population = initialize_population(population_size, num_jobs, num_machines)\n",
        "\n",
        "for generation in range(generations):\n",
        "    fitness = [calculate_makespan_and_balance(individual, job_durations, num_machines) for individual in population]\n",
        "    selected = tournament_selection(population, fitness)\n",
        "    offspring = []\n",
        "    for i in range(0, len(selected), 2):\n",
        "        parent1, parent2 = selected[i], selected[i + 1]\n",
        "        child1, child2 = crossover(parent1, parent2)\n",
        "        offspring.extend([child1, child2])\n",
        "    population = [mutate(individual, num_machines, mutation_rate) for individual in offspring]\n",
        "\n",
        "# Find the best solution and create schedule\n",
        "best_solution = min(population, key=lambda chrom: calculate_makespan_and_balance(chrom, job_durations, num_machines))\n",
        "best_schedule = create_schedule(best_solution, job_durations)\n",
        "print(best_schedule)\n",
        "# Displaying the schedule\n",
        "print(\"Optimal Schedule:\")\n",
        "for machine, jobs in best_schedule.items():\n",
        "    job_list = ', '.join([job[0] for job in jobs])\n",
        "    makespan = sum([job[1] for job in jobs])\n",
        "    print(f\"Machine {machine + 1} - Jobs: {job_list} | Makespan: {makespan} minutes\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_rDkYmvBBTu",
        "outputId": "0936cead-b31e-4dc0-e633-1c025ed996b3"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: [('J4', 8), ('J9', 8)], 1: [('J1', 10), ('J5', 1), ('J7', 3)], 2: [('J2', 3), ('J6', 10), ('J10', 1)], 3: [('J8', 20)], 4: [('J3', 20)]}\n",
            "Optimal Schedule:\n",
            "Machine 1 - Jobs: J4, J9 | Makespan: 16 minutes\n",
            "Machine 2 - Jobs: J1, J5, J7 | Makespan: 14 minutes\n",
            "Machine 3 - Jobs: J2, J6, J10 | Makespan: 14 minutes\n",
            "Machine 4 - Jobs: J8 | Makespan: 20 minutes\n",
            "Machine 5 - Jobs: J3 | Makespan: 20 minutes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Revised approach to include the robot cell information in the schedule\n",
        "\n",
        "# Original schedule\n",
        "original_schedule = best_schedule\n",
        "\n",
        "# User input for robot cells\n",
        "user_input = {\n",
        "    \"R 1\": [1, 3],\n",
        "    \"R 2\": [2,5],\n",
        "    \"R 3\": [4]\n",
        "\n",
        "}\n",
        "\n",
        "# Function to find the robot cell for a given machine\n",
        "def find_robot_cell(machine_number, user_input):\n",
        "    for cell_name, machines in user_input.items():\n",
        "        if machine_number in machines:\n",
        "            return cell_name\n",
        "    return None\n",
        "\n",
        "# Adding robot cell information to each schedule\n",
        "for machine, jobs in original_schedule.items():\n",
        "    robot_cell = find_robot_cell(machine + 1, user_input)  # +1 because machine numbering starts from 1\n",
        "    original_schedule[machine] = (robot_cell, jobs)\n",
        "\n",
        "# Re-arranging the schedule by robot cell\n",
        "rearranged_schedule = dict(sorted(original_schedule.items(), key=lambda item: item[1][0]))\n",
        "\n",
        "# Displaying the rearranged schedule\n",
        "for machine, (cell, jobs) in rearranged_schedule.items():\n",
        "    job_list = ', '.join([job[0] for job in jobs])\n",
        "    makespan = sum([job[1] for job in jobs])\n",
        "    print(f\"Machine {machine} (in {cell}) - Jobs: {job_list} | Makespan: {makespan} minutes\")\n",
        "\n",
        "# Return rearranged_schedule for further analysis if needed\n",
        "rearranged_schedule\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmzGckmN-aln",
        "outputId": "104a36f1-fc40-47fc-dbc9-84cab62f13a4"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Machine 0 (in R 1) - Jobs: J4, J9 | Makespan: 16 minutes\n",
            "Machine 2 (in R 1) - Jobs: J2, J6, J10 | Makespan: 14 minutes\n",
            "Machine 1 (in R 2) - Jobs: J1, J5, J7 | Makespan: 14 minutes\n",
            "Machine 4 (in R 2) - Jobs: J3 | Makespan: 20 minutes\n",
            "Machine 3 (in R 3) - Jobs: J8 | Makespan: 20 minutes\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: ('R 1', [('J4', 8), ('J9', 8)]),\n",
              " 2: ('R 1', [('J2', 3), ('J6', 10), ('J10', 1)]),\n",
              " 1: ('R 2', [('J1', 10), ('J5', 1), ('J7', 3)]),\n",
              " 4: ('R 2', [('J3', 20)]),\n",
              " 3: ('R 3', [('J8', 20)])}"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Original schedule with jobs\n",
        "original_schedule = rearranged_schedule\n",
        "\n",
        "\n",
        "# User input for subtasks of each job\n",
        "job_subtasks = {\n",
        "    'J1': [('T1', 4), ('T2', 3), ('T3', 3)],\n",
        "    'J2': [('T1', 3)],\n",
        "    'J3': [('T1', 7), ('T2', 8), ('T3', 5)],\n",
        "    'J4': [('T1', 4), ('T2', 4)],\n",
        "    'J5': [('T1', 1)],\n",
        "    'J6': [('T1', 5), ('T2', 5)],\n",
        "    'J7': [('T1', 1), ('T2', 1), ('T3', 1)],\n",
        "    'J8': [('T1', 10), ('T2', 10)],\n",
        "    'J9': [('T1', 4), ('T2', 4)],\n",
        "    'J10': [('T1', 1)]\n",
        "}\n",
        "\n",
        "\n",
        "# Replace jobs with their corresponding subtasks in the schedule\n",
        "for machine, (cell, jobs) in original_schedule.items():\n",
        "    new_jobs = []\n",
        "    for job, _ in jobs:\n",
        "        if job in job_subtasks:\n",
        "            for subtask in job_subtasks[job]:\n",
        "                new_subtask = (f\"{subtask[0]}{job[1:]}\", subtask[1])  # Format: T12 for Task 2 of Job 1\n",
        "                new_jobs.append(new_subtask)\n",
        "    original_schedule[machine] = (cell, new_jobs)\n",
        "\n",
        "# Displaying the updated schedule\n",
        "for machine, (cell, jobs) in original_schedule.items():\n",
        "    job_list = ', '.join([f\"{job[0]} ({job[1]})\" for job in jobs])\n",
        "    print(f\"Machine {machine} (in {cell}) - Tasks: {job_list}\")\n",
        "\n",
        "# Return original_schedule for further analysis if needed\n",
        "original_schedule\n",
        "\n"
      ],
      "metadata": {
        "id": "FVB1qVAgIS0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-defining the original schedule and job subtasks due to code execution state reset\n",
        "\n",
        "\n",
        "# User input for subtasks of each job\n",
        "job_subtasks = {\n",
        "    'J1': [('T1', 4), ('T2', 3), ('T3', 3)],\n",
        "    'J2': [('T1', 3)],\n",
        "    'J3': [('T1', 7), ('T2', 8), ('T3', 5)],\n",
        "    'J4': [('T1', 4), ('T2', 4)],\n",
        "    'J5': [('T1', 1)],\n",
        "    'J6': [('T1', 5), ('T2', 5)],\n",
        "    'J7': [('T1', 1), ('T2', 1), ('T3', 1)],\n",
        "    'J8': [('T1', 10), ('T2', 10)],\n",
        "    'J9': [('T1', 4), ('T2', 4)],\n",
        "    'J10': [('T1', 1)]\n",
        "}\n",
        "\n",
        "# Replace jobs with their corresponding subtasks in the schedule\n",
        "for machine, (cell, jobs) in original_schedule.items():\n",
        "    new_jobs = []\n",
        "    for job, _ in jobs:\n",
        "        if job in job_subtasks:\n",
        "            for subtask in job_subtasks[job]:\n",
        "                subtask_label = f\"T{job[1:] if len(job) > 2 else job[1]}{subtask[0][1]}\"  # Correct format: TXY\n",
        "                new_jobs.append((subtask_label, subtask[1]))\n",
        "    original_schedule[machine] = (cell, new_jobs)\n",
        "\n",
        "# Displaying the updated schedule\n",
        "for machine, (cell, jobs) in original_schedule.items():\n",
        "    job_list = ', '.join([f\"{job[0]} ({job[1]})\" for job in jobs])\n",
        "    print(f\"Machine {machine} (in {cell}) - Tasks: {job_list}\")\n",
        "\n",
        "# Return original_schedule for further analysis if needed\n",
        "original_schedule\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zniCDN6MQO5e",
        "outputId": "2aa0da34-5681-4cde-dea0-f8459e8adfe6"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Machine 0 (in R 1) - Tasks: T81 (10), T82 (10)\n",
            "Machine 1 (in R 2) - Tasks: T51 (1), T61 (5), T62 (5), T71 (1), T72 (1), T73 (1)\n",
            "Machine 2 (in R 1) - Tasks: T11 (4), T12 (3), T13 (3), T21 (3), T101 (1)\n",
            "Machine 3 (in R 3) - Tasks: T31 (7), T32 (8), T33 (5)\n",
            "Machine 4 (in R 2) - Tasks: T41 (4), T42 (4), T91 (4), T92 (4)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: ('R 1', [('T81', 10), ('T82', 10)]),\n",
              " 1: ('R 2',\n",
              "  [('T51', 1), ('T61', 5), ('T62', 5), ('T71', 1), ('T72', 1), ('T73', 1)]),\n",
              " 2: ('R 1', [('T11', 4), ('T12', 3), ('T13', 3), ('T21', 3), ('T101', 1)]),\n",
              " 3: ('R 3', [('T31', 7), ('T32', 8), ('T33', 5)]),\n",
              " 4: ('R 2', [('T41', 4), ('T42', 4), ('T91', 4), ('T92', 4)])}"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-defining the original schedule and job subtasks due to code execution state reset\n",
        "\n",
        "# User input for subtasks of each job\n",
        "job_subtasks = {\n",
        "    'J1': [('T1', 4,'S1'), ('T2', 3,'S3'), ('T3', 3),'S4'],\n",
        "    'J2': [('T1', 3,'S1')],\n",
        "    'J3': [('T1', 7,'S3'), ('T2', 8,'S4'), ('T3', 5,'S6')],\n",
        "    'J4': [('T1', 4,'S1'), ('T2', 4,'S3')],\n",
        "    'J5': [('T1', 1,'S1')],\n",
        "    'J6': [('T1', 5,'S2'), ('T2', 5,'S3')],\n",
        "    'J7': [('T1', 1,'S2'), ('T2', 1,'S2'), ('T3', 1,'S1')],\n",
        "    'J8': [('T1', 10,'S4'), ('T2', 10,'S2')],\n",
        "    'J9': [('T1', 4,'S1'), ('T2', 4,'S4')],\n",
        "    'J10': [('T1', 1,'S4')]\n",
        "}\n",
        "\n",
        "# Replace jobs with their corresponding subtasks and tools in the schedule\n",
        "for machine, (cell, jobs) in original_schedule.items():\n",
        "    new_jobs = []\n",
        "    for job_info in jobs:\n",
        "        job, duration = job_info[0], job_info[1]  # Unpack job ID and duration\n",
        "        if job in job_subtasks:\n",
        "            for subtask in job_subtasks[job]:\n",
        "                # Extract task number and job number more safely\n",
        "                task_number = subtask[0][1:]  # Assuming task format is 'T<number>'\n",
        "                job_number = job[1:]  # Extracting job number from job ID\n",
        "                subtask_label = f\"T{job_number}{task_number}\"  # Correct format: TXY\n",
        "\n",
        "                tool = subtask[2] if len(subtask) > 2 else 'None'  # Handling missing tool info\n",
        "                new_jobs.append((subtask_label, subtask[1], tool))\n",
        "    original_schedule[machine] = (cell, new_jobs)\n",
        "\n",
        "# Displaying the updated schedule with tools\n",
        "for machine, (cell, jobs) in original_schedule.items():\n",
        "    job_list = ', '.join([f\"{job[0]} ({job[1]} units, Tool: {job[2]})\" for job in jobs])\n",
        "    print(f\"Machine {machine} (in {cell}) - Tasks: {job_list}\")\n",
        "\n",
        "# Return original_schedule for further analysis if needed\n",
        "original_schedule\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0DxQyd0fAGB",
        "outputId": "ba712b0d-dd2c-414c-8881-5217046041cc"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Machine 0 (in R 1) - Tasks: T41 (4 units, Tool: S1), T42 (4 units, Tool: S3), T91 (4 units, Tool: S1), T92 (4 units, Tool: S4)\n",
            "Machine 1 (in R 2) - Tasks: T11 (4 units, Tool: S1), T12 (3 units, Tool: S3), T13 (3 units, Tool: None), T1 (4 units, Tool: None), T51 (1 units, Tool: S1), T71 (1 units, Tool: S2), T72 (1 units, Tool: S2), T73 (1 units, Tool: S1)\n",
            "Machine 2 (in R 1) - Tasks: T21 (3 units, Tool: S1), T61 (5 units, Tool: S2), T62 (5 units, Tool: S3), T101 (1 units, Tool: S4)\n",
            "Machine 3 (in R 3) - Tasks: T81 (10 units, Tool: S4), T82 (10 units, Tool: S2)\n",
            "Machine 4 (in R 2) - Tasks: T31 (7 units, Tool: S3), T32 (8 units, Tool: S4), T33 (5 units, Tool: S6)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: ('R 1',\n",
              "  [('T41', 4, 'S1'), ('T42', 4, 'S3'), ('T91', 4, 'S1'), ('T92', 4, 'S4')]),\n",
              " 1: ('R 2',\n",
              "  [('T11', 4, 'S1'),\n",
              "   ('T12', 3, 'S3'),\n",
              "   ('T13', 3, 'None'),\n",
              "   ('T1', '4', 'None'),\n",
              "   ('T51', 1, 'S1'),\n",
              "   ('T71', 1, 'S2'),\n",
              "   ('T72', 1, 'S2'),\n",
              "   ('T73', 1, 'S1')]),\n",
              " 2: ('R 1',\n",
              "  [('T21', 3, 'S1'), ('T61', 5, 'S2'), ('T62', 5, 'S3'), ('T101', 1, 'S4')]),\n",
              " 3: ('R 3', [('T81', 10, 'S4'), ('T82', 10, 'S2')]),\n",
              " 4: ('R 2', [('T31', 7, 'S3'), ('T32', 8, 'S4'), ('T33', 5, 'S6')])}"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Original schedule with robotic cells and tasks\n",
        "\n",
        "\n",
        "# Filter function for a specific robotic cell\n",
        "def filter_schedule_by_robot_cell(schedule, cell_name):\n",
        "    return {machine: (cell, tasks) for machine, (cell, tasks) in schedule.items() if cell == cell_name}\n",
        "\n",
        "# Example: Filtering for Robotic Cell R1\n",
        "filtered_schedule_R1 = filter_schedule_by_robot_cell(original_schedule, 'R 1')\n",
        "\n",
        "# Displaying the filtered schedule for R1\n",
        "for machine, (cell, tasks) in filtered_schedule_R1.items():\n",
        "    task_list = ', '.join([f\"{task[0]} ({task[1]})\" for task in tasks])\n",
        "    print(f\"Machine {machine} (in {cell}) - Tasks: {task_list}\")\n",
        "\n",
        "# Return filtered_schedule_R1 for further analysis if needed\n",
        "filtered_schedule_R1\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsppfoMgRTdE",
        "outputId": "9b1b32df-f00b-4467-c8e2-b91a74eca63a"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Machine 0 (in R 1) - Tasks: T41 (4), T42 (4), T91 (4), T92 (4)\n",
            "Machine 2 (in R 1) - Tasks: T21 (3), T61 (5), T62 (5), T101 (1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: ('R 1',\n",
              "  [('T41', 4, 'S1'), ('T42', 4, 'S3'), ('T91', 4, 'S1'), ('T92', 4, 'S4')]),\n",
              " 2: ('R 1',\n",
              "  [('T21', 3, 'S1'), ('T61', 5, 'S2'), ('T62', 5, 'S3'), ('T101', 1, 'S4')])}"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_makespan_and_tool_change(schedule):\n",
        "    machines = {}\n",
        "    makespan = 0\n",
        "    tool_changeover_time = 0\n",
        "\n",
        "    for _, (job, tasks) in schedule.items():\n",
        "        current_tool = None\n",
        "        current_machine = None\n",
        "        machine_time = 0\n",
        "\n",
        "        for task in tasks:\n",
        "            task_id, processing_time, tool = task\n",
        "\n",
        "            if current_tool is None:\n",
        "                current_tool = tool\n",
        "                current_machine = job + current_tool\n",
        "                machines[current_machine] = 0\n",
        "\n",
        "            if current_tool != tool:\n",
        "                tool_changeover_time += 5\n",
        "                current_tool = tool\n",
        "\n",
        "            if current_machine != job + current_tool:\n",
        "                machine_time = machines.get(current_machine, 0)\n",
        "                current_machine = job + current_tool\n",
        "\n",
        "            machine_time += processing_time\n",
        "            machines[current_machine] = machine_time\n",
        "            makespan = max(makespan, machine_time)\n",
        "\n",
        "    return makespan, tool_changeover_time\n",
        "\n",
        "schedule = filtered_schedule_R1\n",
        "\n",
        "makespan, tool_changeover_time = calculate_makespan_and_tool_change(schedule)\n",
        "\n",
        "print(\"Makespan:\", makespan)\n",
        "print(\"Tool Changeover Time:\", tool_changeover_time)\n",
        "\n",
        "# Remove 'R 1' from each schedule entry\n",
        "new_schedule = {key: value[1] for key, value in schedule.items()}\n",
        "\n",
        "print(new_schedule)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUReSS-hod8h",
        "outputId": "8da9b615-376d-4a6a-8803-e4ada4c8624a"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Makespan: 16\n",
            "Tool Changeover Time: 30\n",
            "{0: [('T41', 4, 'S1'), ('T42', 4, 'S3'), ('T91', 4, 'S1'), ('T92', 4, 'S4')], 2: [('T21', 3, 'S1'), ('T61', 5, 'S2'), ('T62', 5, 'S3'), ('T101', 1, 'S4')]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "schedule = {\n",
        "    0: ('R 1', [('T41', 4, 'S1'), ('T42', 4, 'S3'), ('T91', 4, 'S1'), ('T92', 4, 'S4')]),\n",
        "    2: ('R 1', [('T21', 3, 'S1'), ('T61', 5, 'S2'), ('T62', 5, 'S3'), ('T101', 1, 'S4')])\n",
        "}\n",
        "\n",
        "# Remove 'R 1' from each schedule entry\n",
        "new_schedule = {key: value[1] for key, value in schedule.items()}\n",
        "\n",
        "print(new_schedule)\n"
      ],
      "metadata": {
        "id": "y0m1qZ_SxeLu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}